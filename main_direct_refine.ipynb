{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 16:48:41.558986: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-27 16:48:41.717216: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-27 16:48:42.412253: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-27 16:48:42.412335: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-07-27 16:48:42.412342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93002320935c4d7bb1ee18d3fa32ab68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT event_type FROM event_record\n",
      "Debug: Initial chatbot: [('', '')]\n",
      "Debug: Response in execute_sql: SELECT event_type FROM event_record\n",
      "Debug: Chatbot before SQL execution: [('', '')]\n",
      "Debug: Chatbot before updating: [('', '')]\n",
      "Debug: Chatbot after updating: [('', \"\\n\\n====================\\n\\nsql语句执行成功,结果如下:\\n\\n[('打架',), ('聚众',), ('翻越',), ('打架',), ('聚众',), ('翻越',), ('打架',), ('聚众',), ('翻越',)]\")]\n",
      "SELECT event_type FROM event_record\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import gradio as gr\n",
    "import mdtex2html\n",
    "import platform\n",
    "from utility.utils import config_dict\n",
    "from utility.loggers import logger\n",
    "from sentence_transformers import util\n",
    "from local_database import db_operate\n",
    "from utils import obtain_sql, retrieval_related_table, execute_sql\n",
    "from prompt import query_template, chatbot_prompt\n",
    "\n",
    "# Directly specify the device to use cuda:5\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data1/dxw_data/llm/chatglm3-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"/data1/dxw_data/llm/chatglm3-6b\", trust_remote_code=True).half().to(device)\n",
    "model = model.eval()\n",
    "\n",
    "def parse_text(text):\n",
    "    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\"+line\n",
    "    text = \"\".join(lines)\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "def filter_input(input_text):\n",
    "    \"\"\"Function to filter out specified characters from the input text.\"\"\"\n",
    "    filtered_text = input_text.replace(\"你好\", \"\")\n",
    "    return filtered_text\n",
    "\n",
    "def predict(input, history):\n",
    "    max_length = 2048\n",
    "    top_p = 0.7\n",
    "    temperature = 0.2\n",
    "    dboperate = db_operate(config_dict['db_path'])\n",
    "    input_prompt = chatbot_prompt\n",
    "    input_prompt = retrieval_related_table(input_prompt, input, history, top_k=3)\n",
    "    input_prompt += query_template\n",
    "    query = input_prompt.replace(\"<user_input>\", input)\n",
    "    \n",
    "    response, history = model.chat(tokenizer, query, history=history, max_length=max_length, top_p=top_p, temperature=temperature)\n",
    "    \n",
    "    response = parse_text(response)\n",
    "    response = obtain_sql(response)\n",
    "    \n",
    "    chatbot = [(\"\", \"\")]  # Initialize chatbot list with an empty entry\n",
    "    print(f\"Debug: Initial chatbot: {chatbot}\")  # Print initial chatbot list\n",
    "    chatbot = execute_sql(response, chatbot, dboperate)\n",
    "    \n",
    "    return response, history\n",
    "\n",
    "# Simulate user input and history\n",
    "user_input = \"你好，请帮我查询所有的事件类型\"\n",
    "history = []\n",
    "\n",
    "# Apply the filter to the user input\n",
    "filtered_input = filter_input(user_input)\n",
    "\n",
    "# Call the predict function with the filtered input\n",
    "response, history = predict(filtered_input, history)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
