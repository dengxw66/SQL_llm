{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import gradio as gr\n",
    "import mdtex2html\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from utility.utils import config_dict\n",
    "from utility.loggers import logger\n",
    "from sentence_transformers import util\n",
    "from local_database import db_operate\n",
    "from utils import obtain_sql, retrieval_related_table, execute_sql\n",
    "from prompt import query_template, chatbot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data1/dxw_data/llm/chatglm3-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"/data1/dxw_data/llm/chatglm3-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Override Chatbot.postprocess\"\"\"\n",
    "\n",
    "def postprocess(self, y):\n",
    "    if y is None:\n",
    "        return []\n",
    "    for i, (message, response) in enumerate(y):\n",
    "        y[i] = (\n",
    "            None if message is None else mdtex2html.convert((message)),\n",
    "            None if response is None else mdtex2html.convert(response),\n",
    "        )\n",
    "    return y\n",
    "\n",
    "gr.Chatbot.postprocess = postprocess\n",
    "\n",
    "def parse_text(text):\n",
    "    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\"+line\n",
    "    text = \"\".join(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "def predict(input, chatbot, history):\n",
    "    max_length = 2048\n",
    "    top_p = 0.7\n",
    "    temperature = 0.2\n",
    "    dboperate = db_operate(config_dict['db_path'])\n",
    "    input_prompt = chatbot_prompt\n",
    "    input_prompt = retrieval_related_table(input_prompt, input, history, top_k=3)\n",
    "    input_prompt += query_template\n",
    "    query = input_prompt.replace(\"<user_input>\", input)\n",
    "    chatbot.append((parse_text(input), \"\"))\n",
    "    # 流式输出\n",
    "    # for response, history in model.stream_chat(tokenizer, query, history, max_length=max_length, top_p=top_p,\n",
    "    #                                            temperature=temperature):\n",
    "    #     chatbot[-1] = (parse_text(input), parse_text(response))\n",
    "    response, history = model.chat(tokenizer, query, history=history, max_length=max_length, top_p=top_p,temperature=temperature)\n",
    "    chatbot[-1] = (parse_text(input), parse_text(response))\n",
    "    # chatbot[-1] = (chatbot[-1][0], chatbot[-1][1])\n",
    "    # 获取结果中的SQL语句\n",
    "    response = obtain_sql(response)\n",
    "    chatbot = execute_sql(response, chatbot, dboperate)\n",
    "    return chatbot, history\n",
    "\n",
    "\n",
    "def reset_user_input():\n",
    "    return gr.update(value='')\n",
    "\n",
    "\n",
    "def reset_state():\n",
    "    return [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.50.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/gradio/blocks.py\", line 1185, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/gradio/utils.py\", line 661, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_776067/3569696716.py\", line 62, in predict\n",
      "    response, history = model.chat(tokenizer, query, history=history, max_length=max_length, top_p=top_p,temperature=temperature)\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/dxw/.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py\", line 1035, in chat\n",
      "    outputs = self.generate(**inputs, **gen_kwargs, eos_token_id=eos_token_id)\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1499, in generate\n",
      "    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "  File \"/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1149, in _validate_generated_length\n",
      "    raise ValueError(\n",
      "ValueError: Input length of input_ids is 2707, but `max_length` is set to 2048. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with gr.Blocks(css=\".custom-textbox { margin-bottom: 10px; }\") as demo:\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">🤖未来媒体安防大模型</h1>\"\"\")\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            with gr.Column(scale=12):\n",
    "                user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=10, elem_classes=\"custom-textbox\")\n",
    "            with gr.Column(min_width=32, scale=1):\n",
    "                submitBtn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        with gr.Column(scale=1):\n",
    "            emptyBtn = gr.Button(\"Clear History\")\n",
    "            # max_length = gr.Slider(0, 4096, value=2048, step=1.0, label=\"Maximum length\", interactive=True)\n",
    "            # top_p = gr.Slider(0, 1, value=0.7, step=0.01, label=\"Top P\", interactive=True)\n",
    "            # temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)\n",
    "\n",
    "    history = gr.State([])\n",
    "\n",
    "    submitBtn.click(predict, [user_input, chatbot, history], [chatbot, history], show_progress=True)\n",
    "    submitBtn.click(reset_user_input, [], [user_input])\n",
    "\n",
    "    emptyBtn.click(reset_state, outputs=[chatbot, history], show_progress=True)\n",
    "\n",
    "demo.queue().launch(share=False, inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
