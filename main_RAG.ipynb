{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import gradio as gr\n",
    "import mdtex2html\n",
    "import platform\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from utility.utils import config_dict\n",
    "from utility.loggers import logger\n",
    "from sentence_transformers import util\n",
    "from local_database import db_operate\n",
    "from utils import obtain_sql, retrieval_related_table, execute_sql\n",
    "from prompt import query_template, chatbot_prompt\n",
    "\n",
    "# RAG\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
    "from torch.mps import empty_cache\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLM(LLM):\n",
    "    max_token: int = 2048\n",
    "    temperature: float = 0.8\n",
    "    top_p = 0.9\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history_len: int = 1024\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"GLM\"\n",
    "            \n",
    "    def load_model(self, llm_device=\"gpu\",model_name_or_path=None):\n",
    "        model_config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path, config=model_config, trust_remote_code=True, device='cuda:5').half() # GLM模块装在gpu: 6\n",
    "\n",
    "\n",
    "\n",
    "    def _call(self,prompt:str,history:List[str] = [],stop: Optional[List[str]] = None):\n",
    "        response, _ = self.model.chat(\n",
    "                    self.tokenizer,prompt,\n",
    "                    history=history[-self.history_len:] if self.history_len > 0 else [],\n",
    "                    max_length=self.max_token,temperature=self.temperature,\n",
    "                    top_p=self.top_p)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = TextLoader(\"/data1/dxw_data/SAFE/future_media_llm/GLMSQL/data/daily.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 104, which is longer than the specified 100\n",
      "Created a chunk of size 107, which is longer than the specified 100\n",
      "Created a chunk of size 142, which is longer than the specified 100\n",
      "Created a chunk of size 135, which is longer than the specified 100\n",
      "No sentence-transformers model found with name /data1/dxw_data/llm/text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "model_kwargs = {'device': 'cuda:6'}  # embedding模块装在gpu: 7\n",
    "embedding_function = HuggingFaceEmbeddings(model_name='/data1/dxw_data/llm/text2vec-large-chinese',model_kwargs=model_kwargs) # 会报错“No sentence-transformers model found”但是不影响使用,这只是huggingface的检测问题。\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "modelpath = \"/data1/dxw_data/llm/chatglm3-6b\"\n",
    "sys.path.append(modelpath)\n",
    "llm = GLM()\n",
    "llm.load_model(model_name_or_path = modelpath)\n",
    "tokenizer=llm.tokenizer\n",
    "model=llm.model\n",
    "#---------------------------至此, 成功加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_text(text):\n",
    "    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\"+line\n",
    "    text = \"\".join(lines)\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "def predict(input, history):\n",
    "    max_length = 2048\n",
    "    top_p = 0.7\n",
    "    temperature = 0.2\n",
    "    dboperate = db_operate(config_dict['db_path'])\n",
    "    input_prompt = chatbot_prompt\n",
    "    input_prompt = retrieval_related_table(input_prompt, input, history, top_k=3)\n",
    "    input_prompt += query_template\n",
    "    query = input_prompt.replace(\"<user_input>\", input)\n",
    "    \n",
    "    response, history = model.chat(tokenizer, query, history=history, max_length=max_length, top_p=top_p, temperature=temperature)\n",
    "    \n",
    "    response = parse_text(response)\n",
    "    response = obtain_sql(response)\n",
    "    \n",
    "    chatbot = [(\"\", \"\")]  # 初始化 chatbot 列表并添加一个空条目\n",
    "    print(f\"Debug: Initial chatbot: {chatbot}\")  # 打印初始 chatbot 列表\n",
    "    chatbot = execute_sql(response, chatbot, dboperate)\n",
    "    \n",
    "    return response, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题中的同义词有：激烈的打斗、混战、激烈的争斗、拳脚相向。类似的事件包括：在小巷子里，小明和一群人扭打在一起，场面变得混乱而激烈；小明与一群人发生激烈的争斗，场面变得混乱；小明与一群人激烈的打斗，场面变得混乱。\n"
     ]
    }
   ],
   "source": [
    "user_input = \"请帮我查询是否有人群斗殴事件\"\n",
    "\n",
    "# 使用RAG检索VUCA数据库增强专家知识\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "query = f\"根据文档内容,找到问题({user_input})中的同义词。如果没有同义词，就概括类似的事件，提炼出几个同义词。直接回复同义词\" # 根据业务逻辑\n",
    "result = qa({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "激烈的打斗、混战、激烈的争斗、拳脚相向。混乱场面:小明与一群人扭打、发生激烈的争斗、激烈的打斗。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"请精简下列的回答，仅仅输出几个词语总结：\"+result[\"result\"]\n",
    "# print(context)\n",
    "response1, history = model.chat(tokenizer, context, history=[])\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请问是否有人群斗殴事件发生，包括激烈的打斗、混战、激烈的争斗和拳脚相向等混乱场面，如小明与一群人扭打、发生激烈的争斗和激烈的打斗等。\n"
     ]
    }
   ],
   "source": [
    "context = \"请你根据问题：\"+user_input +f\"。  并根据同义词：{response1}\"+\"  重新归纳出一个全面的问题，覆盖原问题核心词和同义词。请直接输出最终问题\"\n",
    "# print(context)\n",
    "response, history = model.chat(tokenizer, context, history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(DISTINCT event_type) FROM event_record WHERE glm_output LIKE '%激烈的打斗%' OR glm_output LIKE '%混战%' OR glm_output LIKE '%激烈的争斗%' OR glm_output LIKE '%拳脚相向%';\n",
      "Debug: Initial chatbot: [('', '')]\n",
      "Debug: Response in execute_sql: SELECT COUNT(DISTINCT event_type) FROM event_record WHERE glm_output LIKE '%激烈的打斗%' OR glm_output LIKE '%混战%' OR glm_output LIKE '%激烈的争斗%' OR glm_output LIKE '%拳脚相向%';\n",
      "Debug: Chatbot before SQL execution: [('', '')]\n",
      "Debug: Chatbot before updating: [('', '')]\n",
      "Debug: Chatbot after updating: [('', '\\n\\n====================\\n\\nsql语句执行成功,结果如下:\\n\\n[(0,)]')]\n",
      "SELECT COUNT(DISTINCT event_type) FROM event_record WHERE glm_output LIKE '%激烈的打斗%' OR glm_output LIKE '%混战%' OR glm_output LIKE '%激烈的争斗%' OR glm_output LIKE '%拳脚相向%';\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 模拟用户输入和历史记录\n",
    "user_input = response\n",
    "history = []\n",
    "\n",
    "# 调用predict函数\n",
    "response, history = predict(user_input, history)\n",
    "\n",
    "# 打印response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
